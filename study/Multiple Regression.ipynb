{
 "cells": [
  {
   "attachments": {
    "image.png": {
     "image/png": "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"
    }
   },
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![image.png](attachment:image.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "where  𝑛  is the number of predictors,  𝛽0  is the intercept, and  𝑦̂   is the so-called \"fitted line\" or the predicted value associated with the dependent variable."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Categorical variables**: they represent categories instead of numerical features\n",
    "\n",
    "To identify categorical variables: A first thing you can do is use the .describe() and .info() methods. .describe() will give you info on the data types (like strings, integers, etc), but even then continuous variables might have been imported as strings, so it's very important to really have a look at your data. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__Question:__ How do we choose important variables\n",
    "\n",
    "- Straight forward selection: try all possible combination with variables and use AIC, BIC etc to choose best.\n",
    "\n",
    "- Forward selection:\n",
    "\n",
    "    1. Start with null model\n",
    "    2. Then one linear model for each separate variables\n",
    "    3. Pick the variable with lowest RSS\n",
    "    4. We then add to that model the variable that results variable selection \n",
    "    in the lowest RSS for the new two-variable model.\n",
    "    5. Repeat this until a stoppage criteria is achieved.\n",
    "- Backward selection\n",
    "\n",
    "- Mixed Selection"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Transforming categorical variables"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "When you want to use categorical variables in regression models, they need to be transformed. There are two approaches to this:\n",
    "- 1) Perform label encoding\n",
    "- 2) Create dummy variables / one-hot-encoding"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Label encoding**`\n",
    "\n",
    "1) Put items into a series with --> pd.series(origin)\n",
    "2) Assign series a \"category\" type with --> cat_origin = origin_series.astype('category')\n",
    "\n",
    "You'll perform label encoding in a way that numerical labels are always between 0 and (number_of_categories)-1.\n",
    "\n",
    "3) Use scikit-learn \n",
    "- from sklearn.preprocessing import LabelEncoder\n",
    "- lb_make = LabelEncoder()\n",
    "\n",
    "4) origin_encoded = lb_make.fit_transform(cat_origin)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Create dummy variables**\n",
    "\n",
    "The idea is to convert each category into a new column, and assign a 1 or 0 to the column. \n",
    "\n",
    "1) pd.get_dummies(cat_origin)\n",
    "\n",
    "The advantage of using dummies is that, whatever algorithm you'll be using, your numerical values cannot be misinterpreted as being continuous. Going forward, it's important to know that for linear regression (and most other algorithms in scikit-learn), **one-hot encoding is required** when adding categorical variables in a regression model!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Dummy Variable Trap**\n",
    "\n",
    "Due to the nature of how dummy variables are created, one variable can be predicted from all of the others. This is known as perfect multicollinearity and it can be a problem for regression. Multicollinearity will be covered in depth later but the basic idea behind perfect multicollinearity is that you can perfectly predict what one variable will be using some combination of the other variables. \n",
    "\n",
    "Fortunately, the dummy variable trap can be avoided by simply dropping one of the dummy variables. You can do this by:  \n",
    "1) Convert columns to dummies and drop first variable: passing ```drop_first=True``` to ```get_dummies()```   \n",
    "2) remove the original columns from our data and add the dummy columns instead: \n",
    "- data = data.drop(['cylinders','model year','origin'], axis=1)  \n",
    "\n",
    "3) Bring it all together\n",
    "- data = pd.concat([data, cyl_dummies, yr_dummies, orig_dummies], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
