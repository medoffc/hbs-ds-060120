{
 "cells": [
  {
   "attachments": {
    "image.png": {
     "image/png": "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"
    }
   },
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![image.png](attachment:image.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "where  𝑛  is the number of predictors,  𝛽0  is the intercept, and  𝑦̂   is the so-called \"fitted line\" or the predicted value associated with the dependent variable."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Categorical variables**: they represent categories instead of numerical features\n",
    "\n",
    "To identify categorical variables: A first thing you can do is use the .describe() and .info() methods. .describe() will give you info on the data types (like strings, integers, etc), but even then continuous variables might have been imported as strings, so it's very important to really have a look at your data. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__Question:__ How do we choose important variables\n",
    "\n",
    "- Straight forward selection: try all possible combination with variables and use AIC, BIC etc to choose best.\n",
    "\n",
    "- Forward selection:\n",
    "\n",
    "    1. Start with null model\n",
    "    2. Then one linear model for each separate variables\n",
    "    3. Pick the variable with lowest RSS\n",
    "    4. We then add to that model the variable that results variable selection \n",
    "    in the lowest RSS for the new two-variable model.\n",
    "    5. Repeat this until a stoppage criteria is achieved.\n",
    "- Backward selection\n",
    "\n",
    "- Mixed Selection"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Transforming categorical variables"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "When you want to use categorical variables in regression models, they need to be transformed. There are two approaches to this:\n",
    "- 1) Perform label encoding\n",
    "- 2) Create dummy variables / one-hot-encoding"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Label encoding**`\n",
    "\n",
    "1) Put items into a series with --> pd.series(origin)   \n",
    "2) Assign series a \"category\" type with --> cat_origin = origin_series.astype('category')\n",
    "\n",
    "You'll perform label encoding in a way that numerical labels are always between 0 and (number_of_categories)-1.\n",
    "\n",
    "3) Use scikit-learn \n",
    "- from sklearn.preprocessing import LabelEncoder\n",
    "- lb_make = LabelEncoder()\n",
    "\n",
    "4) origin_encoded = lb_make.fit_transform(cat_origin)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Create dummy variables**\n",
    "\n",
    "The idea is to convert each category into a new column, and assign a 1 or 0 to the column. \n",
    "\n",
    "1) pd.get_dummies(cat_origin)\n",
    "\n",
    "The advantage of using dummies is that, whatever algorithm you'll be using, your numerical values cannot be misinterpreted as being continuous. Going forward, it's important to know that for linear regression (and most other algorithms in scikit-learn), **one-hot encoding is required** when adding categorical variables in a regression model!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Dummy Variable Trap**\n",
    "\n",
    "Due to the nature of how dummy variables are created, one variable can be predicted from all of the others. This is known as perfect multicollinearity and it can be a problem for regression. Multicollinearity will be covered in depth later but the basic idea behind perfect multicollinearity is that you can perfectly predict what one variable will be using some combination of the other variables. \n",
    "\n",
    "Fortunately, the dummy variable trap can be avoided by simply dropping one of the dummy variables. You can do this by:  \n",
    "1) Convert columns to dummies and drop first variable: passing ```drop_first=True``` to ```get_dummies()```   \n",
    "2) remove the original columns from our data and add the dummy columns instead: \n",
    "- data = data.drop(['cylinders','model year','origin'], axis=1)  \n",
    "\n",
    "3) Bring it all together\n",
    "- data = pd.concat([data, cyl_dummies, yr_dummies, orig_dummies], axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Multicollinearity"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Generally, a correlation with an absolute value around 0.7-0.8 or higher is considered a high correlation. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "One way to see correlation:  \n",
    "> pd.plotting.scatter_matrix(data_pred,figsize  = [9, 9]);  \n",
    "> plt.show()  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "One way to see correlation clearly: HEATMAP   \n",
    "> import seaborn as sns  \n",
    "> sns.heatmap(data_pred.corr(), center=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "One way to see correlation:\n",
    "> data.corr()  \n",
    "> abs(data.corr()) > 0.75"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# save absolute value of correlation matrix as a data frame\n",
    "# converts all values to absolute value\n",
    "# stacks the row:column pairs into a multindex\n",
    "# reset the index to set the multindex to seperate columns\n",
    "# sort values. 0 is the column automatically generated by the stacking\n",
    "\n",
    "df=ames_preprocessed.corr().abs().stack().reset_index().sort_values(0, ascending=False)\n",
    "\n",
    "# zip the variable name columns (Which were only named level_0 and level_1 by default) in a new column named \"pairs\"\n",
    "df['pairs'] = list(zip(df.level_0, df.level_1))\n",
    "\n",
    "# set index to pairs\n",
    "df.set_index(['pairs'], inplace = True)\n",
    "\n",
    "#d rop level columns\n",
    "df.drop(columns=['level_1', 'level_0'], inplace = True)\n",
    "\n",
    "# rename correlation column as cc rather than 0\n",
    "df.columns = ['cc']\n",
    "\n",
    "# drop duplicates. This could be dangerous if you have variables perfectly correlated with variables other than themselves.\n",
    "# for the sake of exercise, kept it in.\n",
    "df.drop_duplicates(inplace=True)\n",
    "\n",
    "df[(df.cc>.75) & (df.cc <1)]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Log Transformations"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "When to use: A log transformation is a very useful tool when you have data that clearly does not follow a normal distribution. Log transformation can help reduce skewness when you have skewed data, and can help reducing variability of data. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**First: model raw data**\n",
    "    \n",
    "> from statsmodels.formula.api import ols\n",
    "\n",
    "> outcome = 'mpg'  \n",
    "> x_cols = ['displacement', 'horsepower', 'weight', 'acceleration']  \n",
    "> predictors = '+'.join(x_cols)  \n",
    "> formula = outcome + '~' + predictors  \n",
    "> model = ols(formula=formula, data=data).fit()  \n",
    "> model.summary()  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Second: Look to see which predictors are non-normal:**\n",
    "   >  pd.plotting.scatter_matrix(data[x_cols], figsize=(10,12))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Third: Log Transformation**\n",
    ">   non_normal = ['displacement', 'horsepower', 'weight']  \n",
    "    for feat in non_normal:  \n",
    "        data[feat] = data[feat].map(lambda x: np.log(x))  \n",
    "\n",
    "OR ANOTHER WAY\n",
    "\n",
    "> data_log = pd.DataFrame([])  \n",
    "> data_log['logdisp'] = np.log(data_pred['displacement'])  \n",
    "> data_log['loghorse'] = np.log(data_pred['horsepower'])  \n",
    "> data_log['logweight'] = np.log(data_pred['weight'])  \n",
    "> data_log.hist(figsize  = [6, 6]);  \n",
    "\n",
    "> pd.plotting.scatter_matrix(data[x_cols], figsize=(10,12));"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Fourth: Model after log transformation**\n",
    "    \n",
    "> outcome = 'mpg'  \n",
    "> x_cols = ['displacement', 'horsepower', 'weight', 'acceleration']  \n",
    "> predictors = '+'.join(x_cols)  \n",
    "> formula = outcome + '~' + predictors  \n",
    "> model = ols(formula=formula, data=data).fit()  \n",
    "> model.summary()  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Outcome: increased our  𝑅2  value of the model!**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Feature Scaling and Normalization"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Why this matters: Often, your dataset will contain features that vary largely in magnitudes. If you leave these magnitudes unchanged, coefficient sizes will fluctuate largely in magnitude as well. This can give the false impression that some variables are less important than others."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Why do we need to use feature scaling?\n",
    "\n",
    "- In order to compare the magnitude of coefficients thus increasing the interpretability of coefficients.\n",
    "- It helps handling disparities in units.\n",
    "- Some models use euclidean distance in their computations.\n",
    "- Some models require features to be on equivalent scales.\n",
    "- In the machine learning space, it helps improve the performance of the model and reducing the values/models from varying widely.\n",
    "- Some algorithms are sensitive to the scale of the data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Good rule of thumb: check your features for normality, and while you're at it, scale your features so they have similar magnitudes, even for a \"simple\" model like linear regression."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Min-max scaling\n",
    "\n",
    "When performing min-max scaling, you can transform x to get the transformed $x'$ by using the formula:\n",
    "\n",
    "$$x' = \\dfrac{x - \\min(x)}{\\max(x)-\\min(x)}$$\n",
    "\n",
    "This way of scaling brings all values between 0 and 1. \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Standardization\n",
    "\n",
    "When \n",
    "\n",
    "$$x' = \\dfrac{x - \\bar x}{\\sigma}$$\n",
    "\n",
    "x' will have mean $\\mu = 0$ and $\\sigma = 1$\n",
    "\n",
    "Note that standardization does not make data $more$ normal, it will just change the mean and the standard error!\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Mean normalization\n",
    "When performing mean normalization, you use the following formula:\n",
    "$$x' = \\dfrac{x - \\text{mean}(x)}{\\max(x)-\\min(x)}$$\n",
    "\n",
    "The distribution will have values between -1 and 1, and a mean of 0.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Unit vector transformation\n",
    " When performing unit vector transformations, you can create a new variable x' with a range [0,1]:\n",
    " \n",
    "$$x'= \\dfrac{x}{{||x||}}$$\n",
    "\n",
    "\n",
    "Recall that the norm of x $||x||= \\sqrt{(x_1^2+x_2^2+...+x_n^2)}$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Examples:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "acc = data_pred['acceleration']\n",
    "logdisp = data_log['logdisp']\n",
    "loghorse = data_log['loghorse']\n",
    "logweight = data_log['logweight']\n",
    "\n",
    "scaled_acc = (acc - min(acc)) / (max(acc) - min(acc))\n",
    "scaled_disp = (logdisp - np.mean(logdisp)) / np.sqrt(np.var(logdisp))\n",
    "scaled_weight = (logweight - np.mean(logweight)) / np.sqrt(np.var(logweight))\n",
    "scaled_horse = (loghorse - np.mean(loghorse)) / (max(loghorse) - min(loghorse))\n",
    "\n",
    "data_cont_scaled = pd.DataFrame([])\n",
    "data_cont_scaled['acc'] = scaled_acc\n",
    "data_cont_scaled['disp'] = scaled_disp\n",
    "data_cont_scaled['horse'] = scaled_horse\n",
    "data_cont_scaled['weight'] = scaled_weight\n",
    "\n",
    "data_cont_scaled.hist(figsize = [6, 6]);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Limit variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cat_data = ames.loc[:, ((ames.dtypes != 'object') & (ames.nunique() > 20))]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Look at histograms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, axes = plt.subplots(nrows=(cat_data.shape[1] // 3), ncols=3, figsize=(16,40))\n",
    "\n",
    "categoricals = [column for column in cat_data.columns if column != 'Id']\n",
    "\n",
    "for col, ax in zip(categoricals, axes.flatten()):\n",
    "    ax.hist(ames[col].dropna(), bins='auto')\n",
    "    ax.set_title(col)\n",
    "    \n",
    "fig.tight_layout()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
