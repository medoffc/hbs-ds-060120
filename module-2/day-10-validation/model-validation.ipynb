{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Bias, Variance, and Model Validation\n",
    "\n",
    "![validation gif from giphy](https://media.giphy.com/media/242wLqQerWkxd6GgHB/giphy.gif)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Learning Objectives\n",
    "\n",
    "- Define _bias_ and _variance_ in the context of machine learning\n",
    "- Recognize how bias and variance are related to under- and over-fitting\n",
    "- Summarize why validation is important\n",
    "- Describe how a train-test split works\n",
    "- Apply a train-test split to a dataset using sklearn\n",
    "\n",
    "Bonus:\n",
    "- Explain why k-fold cross validation is often more robust than a single train-test split\n",
    "- Apply k-fold cross validation to a dataset using sklearn"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To recap what we've done so far this week, and especially to reiterate what we did yesterday, let's look at some generated data and see what adding polynomial terms does to our model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Imports\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "# Setting random seed for reproducibility\n",
    "np.random.seed(1000)\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "from sklearn.model_selection import train_test_split, cross_val_score\n",
    "\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.preprocessing import PolynomialFeatures\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.metrics import r2_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 150 samples from uniform distribution between -2pi and 2pi\n",
    "\n",
    "x = np.random.uniform(-2*np.pi, 2*np.pi, 150)\n",
    "\n",
    "# Creating target (y) - so we know the true relationship between x and y\n",
    "# But - adding some noise (error) with 'np.random'\n",
    "\n",
    "y = np.sin(x) + np.random.normal(loc=0, scale=0.4, size=len(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize it\n",
    "plt.scatter(x, y)\n",
    "\n",
    "plt.ylabel('$\\sin(x)$')\n",
    "plt.xlabel('x values are randomly chosen from $[-2\\pi, 2\\pi]$')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fitting a linear model\n",
    "lr = LinearRegression()\n",
    "lr.fit(x.reshape(-1, 1), y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Grabbing the predicted values\n",
    "y_pred = lr.predict(x.reshape(-1, 1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Scoring our model\n",
    "print(f\"R2 Score: {r2_score(y, y_pred)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize it\n",
    "plt.scatter(x, y) # original data\n",
    "\n",
    "plt.plot(x, y_pred, c='red') # predicted values\n",
    "\n",
    "plt.ylabel('$\\sin(x)$ + noise')\n",
    "plt.xlabel('x values randomly chosen between $-2\\pi$ and $2\\pi$')\n",
    "plt.title(\"Linear Regression\")\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Is this a good model? Well - of course not. It is what we would call **underfit** - it is not complex enough to accurately capture the pattern and predict the target.\n",
    "\n",
    "Let's try again, but now with polynomials!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# For this, we'll need some helper functions\n",
    "# Shoutout to Andy for sending me these\n",
    "\n",
    "def create_poly_dataset(x, degree):\n",
    "    \"\"\"\n",
    "    returning dataset with the given polynomial degree\n",
    "    \"\"\"\n",
    "    # Instantiate the PolynomialFeatures object with given 'degree'\n",
    "    poly = PolynomialFeatures(degree=degree)\n",
    "\n",
    "    # Now transform data to create higher order features\n",
    "    new_data = poly.fit_transform(x.reshape(-1, 1))\n",
    "    return new_data\n",
    "\n",
    "def fit_linear_model(data, y):\n",
    "    \"\"\"\n",
    "    fitting a linear model and printing model details\n",
    "    \"\"\"\n",
    "    np.set_printoptions(precision=4, suppress=True)\n",
    "\n",
    "    if data.ndim == 1:\n",
    "        data = data.reshape(-1, 1)\n",
    "\n",
    "    lr = LinearRegression(fit_intercept=False)\n",
    "    lr.fit(data, y)\n",
    "    print(\"-\"*13)\n",
    "    print(\"Coefficients: \", lr.coef_)\n",
    "    y_pred = lr.predict(data)\n",
    "    print(f\"R-Squared: {lr.score(data, y):.3f}\")\n",
    "    return lr\n",
    "\n",
    "def plot_predict(x, y, model):\n",
    "    \"\"\"\n",
    "    plotting predictions against true values\n",
    "    \"\"\"\n",
    "    plt.scatter(x, y, label='true')\n",
    "    x_pred = np.linspace(x.min(), x.max(), 100)\n",
    "    \n",
    "    # visualize beyond this x range by uncommenting below:\n",
    "#     extra = x.ptp() * .2\n",
    "#     x_pred = np.linspace(x.min() - extra, x.max() + extra, 100)\n",
    "\n",
    "    plt.plot(x_pred, model.predict(create_poly_dataset(x_pred, len(model.coef_)-1)),\n",
    "             label='predicted', c='red')\n",
    "\n",
    "    if len(model.coef_) == 1:\n",
    "        plt.title(f\"{len(model.coef_) - 1} Polynomial Terms \\n (no slope)\")\n",
    "    elif (len(model.coef_) - 1) == 1:\n",
    "        plt.title(f\"{len(model.coef_) - 1} Polynomial Term\")\n",
    "    else:\n",
    "        plt.title(f\"{len(model.coef_) - 1} Polynomial Terms\")\n",
    "\n",
    "    plt.legend()\n",
    "    plt.show()\n",
    "    return"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# visualizing an assortment of polynomial degrees\n",
    "# can visualize each sequential polynomial with `range(n)`\n",
    "for i in [0, 1, 2, 3, 5, 7, 9, 13, 18]:\n",
    "    xi = create_poly_dataset(x, i)\n",
    "    plot_predict(x, y, fit_linear_model(xi, y))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So... which one is best?\n",
    "\n",
    "- \n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "-------"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The Bias-Variance Trade Off\n",
    "\n",
    "<img alt=\"original image from https://rmartinshort.jimdofree.com/2019/02/17/overfitting-bias-variance-and-leaning-curves/\" src=\"images/underfit-goodfit-overfit.png\" width=750, height=350>  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Remember - by modeling, we're assuming that there is some relationship between our X variables (the features in our dataset) and our y variable (the target). Thus, there is some underlying '_true_' function that captures the relationship between X and y, which we are trying to find by modeling. Of course, the actual relationship may be quite complex and not wholly represented in our data - our approximation, aka the model we create, is likely only a simplified estimator of whatever our '_true_' function actually would look like.\n",
    "\n",
    "**Bias**: Error introduced by approximating a real-life problem (which may be extremely complicated) by a much simpler model (because the model is too simple to capture the underlying pattern)\n",
    "\n",
    "**Variance**: Amount by which our model would change if we estimated it using a different training dataset (because the model is over-learning from the training data)\n",
    "\n",
    "**Representation:**\n",
    "\n",
    "<img alt=\"from https://hsto.org/files/281/108/1e9/2811081e9eda44d08f350be5a9deb564.png\" src=\"images/bias-variance.png/\" width=350, height=350>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## How To Minimize Bias and Variance\n",
    "\n",
    "Good news! There are tried and true methods to reducing both bias and variance in our modeling process. Testing different models, trying models on different slices of data, transforming or engineering features - all of these things have a role to play in creating better, more robust models.\n",
    "\n",
    "In particular, we've learned so far that we can evaluate the performance of our models, using a scoring metric, which will help us catch if a model is underfit - if it's performing quite poorly, it probably isn't capturing the relationship in our data! \n",
    "\n",
    "But what about overfitting?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model Validation\n",
    "\n",
    "Let's say you have a dataframe, with some number of rows of data, and that's all you have available to you. The hope is that you can train a model on this data that can then be used to make predictions about new data that comes in. You want your model to generalize well and work on this incoming data - not too complex from learning all the details/noise from the data, but also not so simple that the model is useless. How do we do that?\n",
    "\n",
    "<img alt=\"I Love Lucy shrug gif from Giphy\" src=\"https://media.giphy.com/media/JRhS6WoswF8FxE0g2R/giphy.gif\" width=350, height=350>\n",
    "\n",
    "### Train-Test Split\n",
    "\n",
    "The idea: don't train your model on ALL of your data, but keep some of it in reserve to test on, in order to simulate how it will work on new/incoming data.\n",
    "\n",
    "#### Example:\n",
    "\n",
    "<img alt=\"original image from https://www.dataquest.io/wp-content/uploads/kaggle_train_test_split.svg plus some added commentary\" src=\"images/traintestsplit_80-20.png\" width=850, height=150>  \n",
    "\n",
    "Note - here, it looks like we're just taking the tail end of the dataset and setting it aside. In practice (most of the time), the split will randomly choose which rows are in the train vs. test sets."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "How does this fight against overfitting? By witholding data from the training process, we are testing whether the model actually _generalizes_ well. If it does poorly on the test set, it's a good sign that our model learned too much noise from the train set and is overfit! \n",
    "\n",
    "![arrested development gif, found by Andy](https://heavy.com/wp-content/uploads/2013/05/tumblr_mjm9fqhrle1rvnnvyo6_250.gif)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Practice:\n",
    "\n",
    "Let's go back to our Credit data, where we are trying to predict `balance`.\n",
    "\n",
    "Documentation: https://scikit-learn.org/stable/modules/generated/sklearn.model_selection.train_test_split.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('data/Credit.csv',\n",
    "                 usecols=['Income', 'Limit', 'Rating',\n",
    "                          'Cards', 'Age', 'Balance'])\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define our X and y\n",
    "\n",
    "X = df.drop(columns='Balance')\n",
    "y = df.Balance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train test split here!\n",
    "# Set test_size = .33\n",
    "# Set random_state = 42\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y,               # Pass in our X and y\n",
    "                                                    random_state=42,    # Abritary select a random_state \n",
    "                                                    test_size=.33        # Split test size to be 20% of full data.\n",
    "                                                   )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "What did that do?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_test.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(X_train + X_test) == len(X)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now let's put our train/test split into practice:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Instantiate a scaler to scale our data\n",
    "# Let's use Standard Scaler here\n",
    "ss = StandardScaler()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fit our scaler - ON THE TRAINING DATA!!\n",
    "# Then transform both train and test \n",
    "\n",
    "X_train_scaled = ss.fit_transform(X_train)\n",
    "X_test_scaled = ss.transform(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Instantiate an sklearn linear model\n",
    "\n",
    "lr = LinearRegression()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fit your model - ON THE TRAINING DATA!!\n",
    "\n",
    "lr.fit(X_train_scaled, y_train)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Grab predictions for train and test set\n",
    "\n",
    "y_pred_train = lr.predict(X_train_scaled)\n",
    "y_pred_test = lr.predict(X_test_scaled)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# How'd we do?\n",
    "\n",
    "print(f\"Train R2 Score: {lr.score(X_train_scaled,y_train)}\")\n",
    "print(f\"Test R2 Score: {lr.score(X_test_scaled,y_test)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Evaluate!\n",
    "\n",
    "- Since test score < train score, could be a sign of overfitting.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.heatmap(df.corr().abs())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Single variable example\n",
    "\n",
    "X_s = df['Income']\n",
    "y = df['Balance']\n",
    "\n",
    "X_s_train, X_s_test, y_s_train, y_s_test = train_test_split(X_s, y,               # Pass in our X and y\n",
    "                                                    random_state=42,    # Abritary select a random_state \n",
    "                                                    test_size=.33        # Split test size to be 20% of full data.\n",
    "                                                   )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(400,)"
      ]
     },
     "execution_count": 96,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_s.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [],
   "source": [
    "scaler = StandardScaler()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'reshape' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-102-a5112f37950c>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mX_s_train\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mscaler\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit_transform\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_s_train\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mreshape\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m: name 'reshape' is not defined"
     ]
    }
   ],
   "source": [
    "X_s_train = scaler.fit_transform(X_s_train.values,reshape(-1,1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'reshape' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-115-e1c68e80f709>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mX_s_test\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mscaler\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtransform\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_s_test\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mreshape\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m: name 'reshape' is not defined"
     ]
    }
   ],
   "source": [
    "X_s_test = scaler.transform(X_s_test.values,reshape(-1,1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [],
   "source": [
    "lr = LinearRegression()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "Expected 2D array, got 1D array instead:\narray=[ 41.4    22.379  23.012  30.406  69.943  40.885  10.627  75.406  62.413\n 107.841  24.088  19.537  93.039  14.711  51.872  28.575  25.383  64.173\n  20.996  21.011 163.329  63.931  63.095  28.508 146.183  53.48   52.179\n  37.728  20.974  51.345  18.036  27.47   41.868  59.53   28.144  13.676\n  23.857  69.165  30.733  49.502  11.808  14.312  19.636 125.48   26.067\n  26.532  27.847  64.027  30.007  17.392  62.602  58.929  11.187  27.349\n  31.029  38.954  68.462  23.883  27.578  17.055  39.116  55.367  32.856\n  27.794  12.414  12.068  24.23   59.855  80.861  68.206  49.927  29.567\n  63.534  18.145  11.795  15.045  17.765  30.42   33.437  46.007  58.026\n  49.166  24.919  91.362  89.     41.532  20.088  60.449  26.162  76.782\n  85.425  54.663  55.056  10.403  50.699  25.974 104.593  57.337  40.442\n  69.656  60.579  42.471  27.241  39.609 121.709  53.566  19.349  30.002\n  53.308  57.202  16.103  27.229  26.813  19.225  83.948  39.11   22.574\n  65.896  21.455  22.561 158.889  26.37   31.861  23.793  12.    152.298\n  20.918  36.472  53.319  80.616  23.35  186.634  34.772 103.893  18.951\n  30.413  98.515  14.084  39.705  29.705  20.791  82.706  28.316 113.659\n  33.694  23.45   36.508  12.581  55.882  25.936  30.111  21.153  59.879\n  13.234  39.422  36.934  36.295  41.025  21.238  44.646 134.181  29.638\n  23.949  34.48   32.793  27.825  33.214  67.937  16.711  63.809  35.61\n  21.374  30.682  44.473 123.299  15.125  32.164  21.786  19.144  31.811\n  34.95   61.069 182.728  15.476  54.319  21.551  42.529  10.842  39.145\n  27.272 128.669  28.474 135.118 113.772  44.061  39.055  73.914  31.353\n  36.929  12.456  55.187  75.257  33.657  12.031  16.304 106.025  43.479\n  27.369  10.793  20.15   29.725  48.498 149.316  44.205  30.012  27.999\n  15.602  43.682  14.479  15.56   16.751  35.691  41.192  17.316  34.664\n  45.12   14.292  27.59   58.165  15.333  36.362 121.834  58.351  57.872\n  23.365  27.33   10.354  44.522  48.218  10.503  72.945  37.348  92.386\n  12.238  23.106 124.29  140.672  10.735  15.629  14.956  13.444  23.793\n  35.864  12.335  83.851  19.782  11.741  20.405  34.909 148.08   18.701\n  17.7    83.869  58.781  16.819  15.866  13.433  68.713].\nReshape your data either using array.reshape(-1, 1) if your data has a single feature or array.reshape(1, -1) if it contains a single sample.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-105-4c567bd0686f>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mlr\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_s_train\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0my_s_train\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0mlr\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mscore\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_s_test\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0my_s_test\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/lib/python3.7/site-packages/sklearn/linear_model/_base.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, X, y, sample_weight)\u001b[0m\n\u001b[1;32m    490\u001b[0m         \u001b[0mn_jobs_\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mn_jobs\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    491\u001b[0m         X, y = check_X_y(X, y, accept_sparse=['csr', 'csc', 'coo'],\n\u001b[0;32m--> 492\u001b[0;31m                          y_numeric=True, multi_output=True)\n\u001b[0m\u001b[1;32m    493\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    494\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0msample_weight\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/lib/python3.7/site-packages/sklearn/utils/validation.py\u001b[0m in \u001b[0;36mcheck_X_y\u001b[0;34m(X, y, accept_sparse, accept_large_sparse, dtype, order, copy, force_all_finite, ensure_2d, allow_nd, multi_output, ensure_min_samples, ensure_min_features, y_numeric, warn_on_dtype, estimator)\u001b[0m\n\u001b[1;32m    753\u001b[0m                     \u001b[0mensure_min_features\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mensure_min_features\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    754\u001b[0m                     \u001b[0mwarn_on_dtype\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mwarn_on_dtype\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 755\u001b[0;31m                     estimator=estimator)\n\u001b[0m\u001b[1;32m    756\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mmulti_output\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    757\u001b[0m         y = check_array(y, 'csr', force_all_finite=True, ensure_2d=False,\n",
      "\u001b[0;32m~/opt/anaconda3/lib/python3.7/site-packages/sklearn/utils/validation.py\u001b[0m in \u001b[0;36mcheck_array\u001b[0;34m(array, accept_sparse, accept_large_sparse, dtype, order, copy, force_all_finite, ensure_2d, allow_nd, ensure_min_samples, ensure_min_features, warn_on_dtype, estimator)\u001b[0m\n\u001b[1;32m    554\u001b[0m                     \u001b[0;34m\"Reshape your data either using array.reshape(-1, 1) if \"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    555\u001b[0m                     \u001b[0;34m\"your data has a single feature or array.reshape(1, -1) \"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 556\u001b[0;31m                     \"if it contains a single sample.\".format(array))\n\u001b[0m\u001b[1;32m    557\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    558\u001b[0m         \u001b[0;31m# in the future np.flexible dtypes will be handled like object dtypes\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mValueError\u001b[0m: Expected 2D array, got 1D array instead:\narray=[ 41.4    22.379  23.012  30.406  69.943  40.885  10.627  75.406  62.413\n 107.841  24.088  19.537  93.039  14.711  51.872  28.575  25.383  64.173\n  20.996  21.011 163.329  63.931  63.095  28.508 146.183  53.48   52.179\n  37.728  20.974  51.345  18.036  27.47   41.868  59.53   28.144  13.676\n  23.857  69.165  30.733  49.502  11.808  14.312  19.636 125.48   26.067\n  26.532  27.847  64.027  30.007  17.392  62.602  58.929  11.187  27.349\n  31.029  38.954  68.462  23.883  27.578  17.055  39.116  55.367  32.856\n  27.794  12.414  12.068  24.23   59.855  80.861  68.206  49.927  29.567\n  63.534  18.145  11.795  15.045  17.765  30.42   33.437  46.007  58.026\n  49.166  24.919  91.362  89.     41.532  20.088  60.449  26.162  76.782\n  85.425  54.663  55.056  10.403  50.699  25.974 104.593  57.337  40.442\n  69.656  60.579  42.471  27.241  39.609 121.709  53.566  19.349  30.002\n  53.308  57.202  16.103  27.229  26.813  19.225  83.948  39.11   22.574\n  65.896  21.455  22.561 158.889  26.37   31.861  23.793  12.    152.298\n  20.918  36.472  53.319  80.616  23.35  186.634  34.772 103.893  18.951\n  30.413  98.515  14.084  39.705  29.705  20.791  82.706  28.316 113.659\n  33.694  23.45   36.508  12.581  55.882  25.936  30.111  21.153  59.879\n  13.234  39.422  36.934  36.295  41.025  21.238  44.646 134.181  29.638\n  23.949  34.48   32.793  27.825  33.214  67.937  16.711  63.809  35.61\n  21.374  30.682  44.473 123.299  15.125  32.164  21.786  19.144  31.811\n  34.95   61.069 182.728  15.476  54.319  21.551  42.529  10.842  39.145\n  27.272 128.669  28.474 135.118 113.772  44.061  39.055  73.914  31.353\n  36.929  12.456  55.187  75.257  33.657  12.031  16.304 106.025  43.479\n  27.369  10.793  20.15   29.725  48.498 149.316  44.205  30.012  27.999\n  15.602  43.682  14.479  15.56   16.751  35.691  41.192  17.316  34.664\n  45.12   14.292  27.59   58.165  15.333  36.362 121.834  58.351  57.872\n  23.365  27.33   10.354  44.522  48.218  10.503  72.945  37.348  92.386\n  12.238  23.106 124.29  140.672  10.735  15.629  14.956  13.444  23.793\n  35.864  12.335  83.851  19.782  11.741  20.405  34.909 148.08   18.701\n  17.7    83.869  58.781  16.819  15.866  13.433  68.713].\nReshape your data either using array.reshape(-1, 1) if your data has a single feature or array.reshape(1, -1) if it contains a single sample."
     ]
    }
   ],
   "source": [
    "lr.fit(X_s_train,y_s_train)\n",
    "lr.score(X_s_test,y_s_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### But Wait... There's More!\n",
    "\n",
    "Let's change something and see what happens:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for n in range(10):\n",
    "    X_train, X_test, y_train, y_test = train_test_split(X, y, \n",
    "                                                        test_size=0.33, \n",
    "                                                        random_state=n) # <--\n",
    "    scaler = StandardScaler()\n",
    "    X_train_scaled = scaler.fit_transform(X_train)\n",
    "    X_test_scaled = scaler.transform(X_test)\n",
    "    \n",
    "    lr = LinearRegression()\n",
    "    lr.fit(X_train_scaled, y_train)\n",
    "    \n",
    "    y_pred_train = lr.predict(X_train_scaled)\n",
    "    y_pred_test = lr.predict(X_test_scaled)\n",
    "    \n",
    "    print(f\"Random Seed: {n}\")\n",
    "    print(f\"Train R2 Score: {r2_score(y_train, y_pred_train)}\")\n",
    "    print(f\"Test R2 Score: {r2_score(y_test, y_pred_test)}\")\n",
    "    print(\"-----\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "What's happening here? All we're doing is changing our `random_seed` - why is that having such an impact on our model's scores? Some models appear overfit, some don't - and for some, the test score is **better** than our train score!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### K-Fold Cross-Validation\n",
    "\n",
    "Sometimes, random chance means your training data isn't representative, or includes wacky data like all of our outliers. So, why do just one train-test split when you can do `k` number of them!\n",
    "\n",
    "![cross validation image from kaggle: https://www.kaggle.com/alexisbcook/cross-validation](images/cross-validation.png)\n",
    "\n",
    "The good news is, we'll never actually have to do this by hand - `sklearn` will handle it for us!\n",
    "\n",
    "Documentation: https://scikit-learn.org/stable/modules/cross_validation.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Scale our data\n",
    "scaler = StandardScaler()\n",
    "\n",
    "X_scaled = scaler.fit_transform(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Instantiate a fresh linear regression model\n",
    "lr = LinearRegression()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's use cross_val_score\n",
    "# Set cv = 5\n",
    "\n",
    "scores = cross_val_score(lr,X_scaled,y,cv=5,scoring='r2')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0.8603, 0.8555, 0.8882, 0.8733, 0.8846])"
      ]
     },
     "execution_count": 118,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Look at the test scores across our folds\n",
    "scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Scores: 0.872 +/- 0.013\n"
     ]
    }
   ],
   "source": [
    "# Print it nicely\n",
    "print(f\"Scores: {scores.mean():.3f} +/- {scores.std():.3f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### What the score means above:\n",
    "R^2 of the test data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Why show the standard deviation of scores here? I want some measure of the variance among my scores, so I can tell how different my scores were based on different breakdowns of the training data.\n",
    "\n",
    "If I made a change to my model and the average of my cross-validated scores stayed about the same, but the variance among those scores decreased, that's a better, more generalizable model than before!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Additional Resources:\n",
    "\n",
    "- [Great bias/variance infographic](https://elitedatascience.com/bias-variance-tradeoff) from Elite Data Science"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
